{"cells":[{"cell_type":"code","metadata":{"id":"Vu-qMErX5MCE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595157910308,"user_tz":-330,"elapsed":4574,"user":{"displayName":"RAUSHAN KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKPidrEbBhw9gVCzGQ6f3PV8AJpx25Kcnf3mxh=s64","userId":"11148087558451086823"}},"outputId":"eade3ce8-4e93-4c53-9925-0b2df067fac5"},"source":["# Neural networks can be constructed using the torch.nn package.\n","\n","# Define the network\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # 1 input image channel, 6 output channels, 3x3 square convolution\n","        # kernel\n","        self.conv1 = nn.Conv2d(1, 6, 3)\n","        self.conv2 = nn.Conv2d(6, 16, 3)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        print(x.size())\n","        # Max pooling over a (2, 2) window\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","        print(x.size())\n","        # If the size is a square you can only specify a single number\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        print(x.size())\n","        x = x.view(-1, self.num_flat_features(x))\n","        print(x.size())\n","        x = F.relu(self.fc1(x))\n","        print(x.size())\n","        x = F.relu(self.fc2(x))\n","        print(x.size())\n","        x = self.fc3(x)\n","        print(x.size())\n","        return x\n","\n","    def num_flat_features(self, x):\n","        size = x.size()[1:]  # all dimensions except the batch dimension\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features\n","\n","\n","net = Net()\n","print(net)\n","\n","\n","params = list(net.parameters())\n","print(len(params))\n","print(params[0].size())  # conv1's .weight\n","# print('----------------')\n","# for each in params:\n","#   print(each.size())\n","# print('----------------')\n","# print(params[0])\n","# print('**************')\n","# print(params[-1])\n","\n","\n","#Let’s try a random 32x32 input. Note: expected input size of this net (LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32.\n","\n","input = torch.randn(1,1,32,32)\n","print(input)\n","out = net(input)\n","print(out)\n","\n","# Zero the gradient buffers of all parameters and backprops with random gradients:\n","net.zero_grad()\n","out.backward(torch.randn(1, 10))\n","\n","\n","### Loss Function ###\n","# A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n","output = net(input)\n","target = torch.randn(10)  # a dummy target, for example\n","target = target.view(1,-1)  # make it the same shape as output\n","criterion = nn.MSELoss()\n","loss = criterion(output, target)\n","print(loss)\n","\n","print(loss.grad_fn) #MSELoss\n","print(loss.grad_fn.next_functions[0][0])  # Linear\n","print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU\n","\n","### Backprop ###\n","# To backpropagate the error all we have to do is to loss.backward(). \n","# You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n","# Now we shall call loss.backward(), and have a look at conv1’s bias gradients before and after the backward.\n","net.zero_grad() # zeroes the gradient buffers of all parameters\n","print('conv1.bias.grad before backward')\n","print(net.conv2.bias) ##### temp\n","print(net.conv1.bias.grad)\n","\n","loss.backward()\n","\n","print('conv1.bias.grad after backward')\n","print(net.conv1.bias.grad)\n","\n","# Update the weights\n","# (weight = weight - learning_rate * gradient)\n","print(net.parameters)\n","learning_rate = 0.01\n","for f in net.parameters():\n","  f.data.sub_(f.grad.data * learning_rate)\n","\n","# However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n","# To enable this, we built a small package: torch.optim that implements all these methods. Using it is very simple:\n","\n","import torch.optim as optim\n","# create your optimizer\n","optimizer = optim.SGD(net.parameters(), lr=0.01)\n","print(optimizer)\n","# in your training loop:\n","optimizer.zero_grad()  # zero the gradient buffers\n","output = net(input)\n","loss = criterion(output, target)\n","loss.backward()\n","optimizer.step()    # Does the update\n","\n","#print(net.state_dict())\n","\n","# NOTE: Observe how gradient buffers had to be manually set to zero using optimizer.zero_grad().\n","# This is because gradients are accumulated as explained in the Backprop section."],"execution_count":1,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=576, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")\n","10\n","torch.Size([6, 1, 3, 3])\n","tensor([[[[-0.8496,  0.1757,  0.1044,  ..., -0.9030,  0.3680,  0.6075],\n","          [-0.9557,  0.7580,  1.1072,  ...,  1.1732,  0.4037, -0.7146],\n","          [-1.3369,  0.2783, -2.2107,  ...,  0.8581,  1.4845,  0.6992],\n","          ...,\n","          [ 0.1136,  0.8096, -1.1722,  ...,  0.1692, -0.5044,  0.0439],\n","          [ 0.5649, -0.2376, -0.5716,  ..., -0.5036,  1.0726,  1.0586],\n","          [-0.0507, -0.3487,  1.1868,  ..., -1.0833, -1.4617, -0.4981]]]])\n","torch.Size([1, 1, 32, 32])\n","torch.Size([1, 6, 15, 15])\n","torch.Size([1, 16, 6, 6])\n","torch.Size([1, 576])\n","torch.Size([1, 120])\n","torch.Size([1, 84])\n","torch.Size([1, 10])\n","tensor([[-0.0086, -0.0022, -0.0154, -0.0584,  0.0079, -0.0927, -0.0534,  0.0669,\n","         -0.0064, -0.0497]], grad_fn=<AddmmBackward>)\n","torch.Size([1, 1, 32, 32])\n","torch.Size([1, 6, 15, 15])\n","torch.Size([1, 16, 6, 6])\n","torch.Size([1, 576])\n","torch.Size([1, 120])\n","torch.Size([1, 84])\n","torch.Size([1, 10])\n","tensor(0.7431, grad_fn=<MseLossBackward>)\n","<MseLossBackward object at 0x7f8b757e2978>\n","<AddmmBackward object at 0x7f8b1090f240>\n","<AccumulateGrad object at 0x7f8b757e2978>\n","conv1.bias.grad before backward\n","Parameter containing:\n","tensor([ 0.1005,  0.1149, -0.0035, -0.0599,  0.1005, -0.1161, -0.0336,  0.0563,\n","        -0.0379, -0.0995, -0.0797,  0.0719, -0.1312,  0.1273,  0.0848, -0.0677],\n","       requires_grad=True)\n","tensor([0., 0., 0., 0., 0., 0.])\n","conv1.bias.grad after backward\n","tensor([-0.0037, -0.0038, -0.0226, -0.0020,  0.0027,  0.0080])\n","<bound method Module.parameters of Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=576, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")>\n","SGD (\n","Parameter Group 0\n","    dampening: 0\n","    lr: 0.01\n","    momentum: 0\n","    nesterov: False\n","    weight_decay: 0\n",")\n","torch.Size([1, 1, 32, 32])\n","torch.Size([1, 6, 15, 15])\n","torch.Size([1, 16, 6, 6])\n","torch.Size([1, 576])\n","torch.Size([1, 120])\n","torch.Size([1, 84])\n","torch.Size([1, 10])\n"],"name":"stdout"}]}],"metadata":{"colab":{"name":"neural_networks.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP3b7IkF48MfYKnn5fXHPYb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}