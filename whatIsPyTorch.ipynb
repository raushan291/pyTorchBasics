{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"whatIsPyTorch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNeI/qnEC1ZoPJE/y/nbu6d"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"llQDdtVkIMdO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594903308619,"user_tz":-330,"elapsed":3664,"user":{"displayName":"RAUSHAN KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKPidrEbBhw9gVCzGQ6f3PV8AJpx25Kcnf3mxh=s64","userId":"11148087558451086823"}},"outputId":"f83d3187-d46d-4711-96bb-355f34d1cdf5"},"source":["from __future__ import print_function\n","import torch\n","import numpy as np\n","\n","x = torch.empty(5,3)\n","print(x)\n","\n","x = torch.randn(5,3)\n","print(x)\n","\n","x = torch.tensor([3.24, 5])\n","print(x)\n","\n","x = x.new_ones(5,3, dtype=torch.double)    # new_* methods take in sizes\n","print(x)\n","\n","x = torch.randn_like(x, dtype=torch.float)  # override dtype!\n","print(x)                                    # result has the same size \n","\n","print(x.size())\n","\n","y = torch.rand(5,3)\n","print(x+y)\n","print(torch.ones(4,4) + torch.ones_like(torch.ones(4,4)))\n","print(torch.add(x,y))\n","\n","result = torch.empty(5,3)\n","print(result)\n","torch.add(x,y, out=result)\n","print(result)\n","\n","# Addition: in-place\n","# add x to y\n","y.add(x)\n","print(y)\n","# ***NOTE :: Any operation that mutates a tensor in-place is post-fixed with an ``_``. For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.  ***\n","\n","print(x[:, 1])\n","\n","### Resizing: If you want to resize/reshape tensor, you can use torch.view ####\n","x = torch.randn(4,4)\n","y = x.view(16)\n","print(y)\n","z = x.view(-1, 8)     # the size -1 is inferred from other dimensions\n","print(z)\n","print(x.size(), y.size(), z.size())\n","\n","a = torch.arange(1, 16)\n","print(a)\n","\n","\n","# If you have a one element tensor, use .item() to get the value as a Python number\n","x = torch.randn(1)\n","print(x)\n","print(x.item())\n","\n","### NumPy Bridge ::: ###\n","a = torch.ones(5)\n","print(a)\n","b = a.numpy()\n","print(b)\n","\n","### See how the numpy array changed in value.\n","a.add_(1)\n","print(a)\n","print(b)\n","\n","### Converting NumPy Array to Torch Tensor ###\n","#See how changing the np array changed the Torch Tensor automatically\n","\n","a  = np.ones(5)\n","print(a)\n","b = torch.from_numpy(a)\n","print(b)\n","np.add(a, 1, out=a)\n","print(a)\n","print(b)\n","# *** NOTE : All the Tensors on the CPU except a CharTensor support converting to NumPy and back. *** #\n","\n","### CUDA Tensors\n","# Tensors can be moved onto any device using the .to method. #\n","\n","# let us run this cell only if CUDA is available\n","# We will use ``torch.device`` objects to move tensors in and out of GPU\n","if torch.cuda.is_available():\n","  device = torch.device('cuda')\n","  y = torch.ones_like(x, device=device)\n","  x = x.to(device)\n","  z = x+y\n","  print(z)\n","  print(z.to('cpu', torch.double))\n","else:\n","  print('cuda GPU is not available..')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["tensor([[1.7201e-35, 0.0000e+00, 3.3631e-44],\n","        [0.0000e+00,        nan, 0.0000e+00],\n","        [1.1578e+27, 1.1362e+30, 7.1547e+22],\n","        [4.5828e+30, 1.2121e+04, 7.1846e+22],\n","        [9.2198e-39, 7.0374e+22, 0.0000e+00]])\n","tensor([[-0.8533, -2.8618,  0.7062],\n","        [ 0.8210,  1.7726,  1.2756],\n","        [-1.3296,  0.3248, -0.0748],\n","        [ 1.0549,  0.0478,  0.2159],\n","        [-1.6172, -0.7876,  1.2344]])\n","tensor([3.2400, 5.0000])\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]], dtype=torch.float64)\n","tensor([[ 0.6209,  0.8220, -0.4887],\n","        [-0.8286, -0.3417, -0.1079],\n","        [ 0.5071, -0.1091,  0.9209],\n","        [-0.8612, -1.4694,  0.2869],\n","        [-0.3847,  1.1756,  0.3453]])\n","torch.Size([5, 3])\n","tensor([[ 0.8434,  0.8371,  0.3873],\n","        [-0.3823,  0.1815,  0.5889],\n","        [ 1.3245,  0.4214,  1.9161],\n","        [-0.7698, -0.7926,  1.0378],\n","        [-0.2274,  1.9385,  0.9585]])\n","tensor([[2., 2., 2., 2.],\n","        [2., 2., 2., 2.],\n","        [2., 2., 2., 2.],\n","        [2., 2., 2., 2.]])\n","tensor([[ 0.8434,  0.8371,  0.3873],\n","        [-0.3823,  0.1815,  0.5889],\n","        [ 1.3245,  0.4214,  1.9161],\n","        [-0.7698, -0.7926,  1.0378],\n","        [-0.2274,  1.9385,  0.9585]])\n","tensor([[ 1.7204e-35,  0.0000e+00,  3.8726e-01],\n","        [-3.8229e-01,  1.8149e-01,  5.8887e-01],\n","        [ 1.3245e+00,  4.2144e-01,  1.9161e+00],\n","        [-7.6984e-01, -7.9255e-01,  1.0378e+00],\n","        [-2.2742e-01,  1.9385e+00,  9.5847e-01]])\n","tensor([[ 0.8434,  0.8371,  0.3873],\n","        [-0.3823,  0.1815,  0.5889],\n","        [ 1.3245,  0.4214,  1.9161],\n","        [-0.7698, -0.7926,  1.0378],\n","        [-0.2274,  1.9385,  0.9585]])\n","tensor([[0.2225, 0.0151, 0.8759],\n","        [0.4463, 0.5232, 0.6967],\n","        [0.8174, 0.5305, 0.9952],\n","        [0.0913, 0.6768, 0.7509],\n","        [0.1572, 0.7629, 0.6131]])\n","tensor([ 0.8220, -0.3417, -0.1091, -1.4694,  1.1756])\n","tensor([-0.1866, -0.6754, -0.7539,  0.9604, -1.6846,  0.8498,  0.2987,  0.6805,\n","         0.7883, -0.2406, -0.1464,  1.8378,  0.9046, -1.1007, -0.6013, -0.7704])\n","tensor([[-0.1866, -0.6754, -0.7539,  0.9604, -1.6846,  0.8498,  0.2987,  0.6805],\n","        [ 0.7883, -0.2406, -0.1464,  1.8378,  0.9046, -1.1007, -0.6013, -0.7704]])\n","torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n","tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n","tensor([-0.6180])\n","-0.617954432964325\n","tensor([1., 1., 1., 1., 1.])\n","[1. 1. 1. 1. 1.]\n","tensor([2., 2., 2., 2., 2.])\n","[2. 2. 2. 2. 2.]\n","[1. 1. 1. 1. 1.]\n","tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n","[2. 2. 2. 2. 2.]\n","tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n","cuda GPU is not available..\n"],"name":"stdout"}]}]}