{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"autograd_automatic_diffentiation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4EwVQvp9SUH1pJBkfp53c"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"fIfA_H15nnOh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"ok","timestamp":1594890459204,"user_tz":-330,"elapsed":1492,"user":{"displayName":"RAUSHAN KUMAR","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKPidrEbBhw9gVCzGQ6f3PV8AJpx25Kcnf3mxh=s64","userId":"11148087558451086823"}},"outputId":"4239acfe-9ab6-4257-a663-9c1426bbd8fc"},"source":["### AUTOGRAD: AUTOMATIC DIFFERENTIATION\n","\n","import torch\n","\n","# Create a tensor and set requires_grad=True to track computation with it\n","x = torch.ones(2, 2, requires_grad=True)\n","print(x)\n","\n","# Do tensor operation\n","y = x + 2\n","print(y)\n","\n","# y was created as a result of an operation, so it has a grad_fn.\n","print(y.grad_fn)\n","\n","# Do more operation on y\n","z = y*y*3\n","out = z.mean()\n","print(z, out)\n","\n","print(x.requires_grad)\n","\n","#.requires_grad_( ... ) changes an existing Tensor’s requires_grad flag in-place. The input flag defaults to False if not given.\n","a = torch.randn(2,2)\n","a = ((a * 3) / (a - 1))\n","print(a.requires_grad)\n","a.requires_grad_(True)\n","print(a.requires_grad)\n","b = (a*a).sum()\n","print(b.grad_fn)\n","\n","### Gradients ###\n","# Let’s backprop now. Because out contains a single scalar, out.backward() is equivalent to out.backward(torch.tensor(1.)).\n","out.backward()  # i.e  operation on torch.tensor(1.) i.e, in dy/dx ==> x is 1\n","\n","# Print gradients d(out)/dx\n","print('d(out)/dx = ',x.grad)\n","\n","############################################\n","# x=torch.tensor(2.0, requires_grad=True)\n","# print(x)  \n","# y=8*x**4 + 3*x**3 + 7*x**2 + 6*x + 3  \n","# y.backward()  \n","# print('dy/dx  = ', x.grad) \n","############################################\n","\n","# Now let’s take a look at an example of vector-Jacobian product:\n","\n","x = torch.randn(3, requires_grad=True)\n","y = x*2\n","print(y)\n","while y.data.norm() < 1000:\n","  y = y*2 \n","print(y)\n","\n","# Now in this case y is no longer a scalar. torch.autograd could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to backward as argument:\n","v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n","y.backward(v)\n","print(x.grad)\n","\n","# You can also stop autograd from tracking history on Tensors with .requires_grad=True either by wrapping the code block in with torch.no_grad():\n","print(x.requires_grad)\n","print( (x **2 ).requires_grad)\n","with torch.no_grad():\n","  print( (x **2 ).requires_grad)\n","\n","# Or by using .detach() to get a new Tensor with the same content but that does not require gradients:\n","print(x.requires_grad)\n","y = x.detach()\n","print(x.requires_grad)\n","print(y.requires_grad)\n","print(x.eq(y))\n","print(x.eq(y).all())"],"execution_count":51,"outputs":[{"output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n","tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)\n","<AddBackward0 object at 0x7f504437cc88>\n","tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n","True\n","False\n","True\n","<SumBackward0 object at 0x7f4ffa8a56a0>\n","d(out)/dx =  tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n","tensor([ 0.3197, -1.0480,  2.0792], grad_fn=<MulBackward0>)\n","tensor([ 163.7021, -536.5969, 1064.5380], grad_fn=<MulBackward0>)\n","tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n","True\n","True\n","False\n","True\n","True\n","False\n","tensor([True, True, True])\n","tensor(True)\n"],"name":"stdout"}]}]}